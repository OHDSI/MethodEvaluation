% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Metrics.R
\name{computeOhdsiBenchmarkMetrics}
\alias{computeOhdsiBenchmarkMetrics}
\title{Generate performance metrics for the OHDSI Methods Benchmark}
\usage{
computeOhdsiBenchmarkMetrics(
  exportFolder,
  mdrr = 1.25,
  stratum = "All",
  trueEffectSize = "Overall",
  calibrated = FALSE,
  comparative = FALSE
)
}
\arguments{
\item{exportFolder}{The folder containing the CSV files created using the
\code{\link{packageOhdsiBenchmarkResults}} function. This folder can contain
results from various methods, analyses, and databases.}

\item{mdrr}{The minimum detectable relative risk (MDRR). Only controls with this MDRR
will be used to compute the performance metrics. Set to "All" to include all
controls.}

\item{stratum}{The stratum for which to compute the metrics, e.g. 'Acute Pancreatitis'. Set
to 'All' to use all controls.}

\item{trueEffectSize}{Should the analysis be limited to a specific true effect size? Set to
"Overall" to include all.}

\item{calibrated}{Should confidence intervals and p-values be empirically calibrated before
computing the metrics?}

\item{comparative}{Should the methods be evaluated on the task of comprative effect estimation?
If FALSE, they will be evaluated on the task of effect estimation.}
}
\value{
A data frame with the various metrics per method - analysisId - database combination.
}
\description{
Generate performance metrics for the OHDSI Methods Benchmark
}
